{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce452722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix ,roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.utils import resample\n",
    "import datetime\n",
    "\n",
    "# Load the dataset using pandas\n",
    "data = pd.read_csv('creditcard.csv')\n",
    "X = data.drop('Class', axis=1).values\n",
    "y = data['Class'].values\n",
    "\n",
    "\n",
    "# Set the aesthetic style of the plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create subplots with shared x-axis\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12, 6), gridspec_kw={'height_ratios': [1, 1]})\n",
    "\n",
    "bins = 100\n",
    "\n",
    "# Plotting for Fraud transactions\n",
    "ax1.hist(data.Time[data.Class == 1], bins=bins, color='red', alpha=0.7)\n",
    "ax1.set_title('Fraud', fontsize=14)\n",
    "ax1.set_ylabel('Number of Transactions', fontsize=12)\n",
    "ax1.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "# Set y-axis ticks with a step of 5\n",
    "ax1.set_yticks(np.arange(0, ax1.get_ylim()[1], 5))\n",
    "\n",
    "# Plotting for Normal transactions\n",
    "ax2.hist(data.Time[data.Class == 0], bins=bins, color='blue', alpha=0.7)\n",
    "ax2.set_title('Normal', fontsize=14)\n",
    "ax2.set_xlabel('Time (in Seconds)', fontsize=12)\n",
    "ax2.set_ylabel('Number of Transactions', fontsize=12)\n",
    "ax2.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "# Remove top and right spines\n",
    "for ax in [ax1, ax2]:\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Adjust layout to prevent clipping of y-axis labels\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot([data[data[\"Class\"]==1][\"Amount\"], data[data[\"Class\"]==0][\"Amount\"]], \n",
    "            labels=['Fraud', 'Normal'])\n",
    "\n",
    "plt.title('Transaction Amounts: Fraud vs Normal')\n",
    "plt.ylabel('Amount')\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "# Dataset exploring\n",
    "print(data.columns)\n",
    "\n",
    "# Print the shape of the data\n",
    "data = data.sample(frac=0.1, random_state=1)\n",
    "print(data.shape)\n",
    "print(data.describe())\n",
    "\n",
    "# V1 - V28 are the results of PCA Dimensionality reduction to protect user identities and sensitive features\n",
    "\n",
    "# Plot histograms of each parameter\n",
    "data.hist(figsize=(20, 20))\n",
    "plt.show()\n",
    "\n",
    "# Determine the number of fraud cases in the dataset\n",
    "Fraud = data[data['Class'] == 1]\n",
    "Valid = data[data['Class'] == 0]\n",
    "\n",
    "outlier_fraction = len(Fraud) / float(len(Valid))\n",
    "print(outlier_fraction)\n",
    "\n",
    "print('Fraud Cases: {}'.format(len(data[data['Class'] == 1])))\n",
    "print('Valid Transactions: {}'.format(len(data[data['Class'] == 0])))\n",
    "\n",
    "# Correlation matrix\n",
    "corrmat = data.corr()\n",
    "fig = plt.figure(figsize=(12, 9))\n",
    "\n",
    "sns.heatmap(corrmat, vmax=.8, square=True)\n",
    "plt.show()\n",
    "\n",
    "# Get all the columns from the DataFrame\n",
    "columns = data.columns.tolist()\n",
    "\n",
    "# Filter the columns to remove data we do not want\n",
    "columns = [c for c in columns if c not in [\"Class\"]]\n",
    "\n",
    "# Store the variable we'll be predicting on\n",
    "target = \"Class\"\n",
    "\n",
    "X = data[columns]\n",
    "y = data[target]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Combine X_train and y_train for oversampling\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Separate minority and majority classes\n",
    "minority_class = train_data[train_data['Class'] == 1]\n",
    "majority_class = train_data[train_data['Class'] == 0]\n",
    "\n",
    "# Upsample minority class using SMOTE\n",
    "minority_upsampled = resample(minority_class, replace=True, n_samples=len(majority_class), random_state=42)\n",
    "\n",
    "# Combine oversampled minority class with majority class\n",
    "upsampled_train_data = pd.concat([majority_class, minority_upsampled], axis=0)\n",
    "\n",
    "# Shuffle the data\n",
    "upsampled_train_data = upsampled_train_data.sample(frac=1, random_state=42)\n",
    "\n",
    "# Split the oversampled data back into features (X_resampled) and labels (y_resampled)\n",
    "X_train_resampled = upsampled_train_data.drop('Class', axis=1)\n",
    "y_train_resampled = upsampled_train_data['Class']\n",
    "\n",
    "# Define the fitness function for the genetic algorithm\n",
    "def evaluate_features(features):\n",
    "    clf = RandomForestClassifier(n_estimators=5, random_state=5)\n",
    "    clf.fit(X_train_resampled.iloc[:, features], y_train_resampled)\n",
    "    y_pred = clf.predict(X_test.iloc[:, features])\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "# Genetic Algorithm Configuration\n",
    "population_size = 3\n",
    "num_generations = 1\n",
    "num_features = X_train_resampled.shape[1]\n",
    "mutation_rate = 0.1\n",
    "\n",
    "# Initialize the population with random features\n",
    "population = [random.sample(range(num_features), num_features) for _ in range(population_size)]\n",
    "\n",
    "# ...\n",
    "\n",
    "# Genetic Algorithm\n",
    "for generation in range(num_generations):\n",
    "    # Evaluate the fitness of each individual in the population\n",
    "    fitness_scores = [evaluate_features(individual) for individual in population]\n",
    "\n",
    "    # Select the top-performing individuals\n",
    "    num_parents = int(population_size * 0.2)\n",
    "    parents = np.argsort(fitness_scores)[-num_parents:]\n",
    "\n",
    "    # Create a new population by crossing over and mutating parents\n",
    "    new_population = []\n",
    "\n",
    "    for _ in range(population_size - num_parents):\n",
    "        parent1 = random.choice(parents)\n",
    "        parent2 = random.choice(parents)\n",
    "        crossover_point = random.randint(0, num_features - 1)\n",
    "        child = population[parent1][:crossover_point] + population[parent2][crossover_point:]\n",
    "\n",
    "        # Apply mutation\n",
    "        if random.random() < mutation_rate:\n",
    "            mutated_gene = random.randint(0, num_features - 1)\n",
    "            child[mutated_gene] = 1 - child[mutated_gene]\n",
    "\n",
    "        new_population.append(child)\n",
    "\n",
    "    # Replace the old population with the new population\n",
    "    population = population[:num_parents] + new_population\n",
    "\n",
    "    # Train the RandomForestClassifier on the best individual of this generation\n",
    "    best_individual = population[np.argmax(fitness_scores)]\n",
    "    clf_rf = RandomForestClassifier(n_estimators=5, random_state=5)\n",
    "    clf_rf.fit(X_train_resampled.iloc[:, best_individual], y_train_resampled)\n",
    "\n",
    "# Get the best features selected\n",
    "best_features = [i for i, selected in enumerate(best_individual) if selected]\n",
    "\n",
    "# Get feature importances from the trained RandomForestClassifier model\n",
    "feature_importances = clf_rf.feature_importances_\n",
    "\n",
    "# Create a DataFrame with feature names and their importances\n",
    "feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})\n",
    "\n",
    "# Sort the DataFrame by importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance_df)\n",
    "plt.title('Feature Importances')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Train and test a Decision Tree model using the best features\n",
    "clf_decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "clf_decision_tree.fit(X_train_resampled.iloc[:, best_features], y_train_resampled)\n",
    "y_pred_decision_tree = clf_decision_tree.predict(X_test.iloc[:, best_features])\n",
    "\n",
    "# Evaluate Decision Tree model\n",
    "accuracy_decision_tree = accuracy_score(y_test, y_pred_decision_tree)\n",
    "precision_decision_tree = precision_score(y_test, y_pred_decision_tree)\n",
    "recall_decision_tree =recall_score(y_test, y_pred_decision_tree)\n",
    "f1score_decision_tree = f1_score(y_test, y_pred_decision_tree)\n",
    "AUC_decision_tree = roc_auc_score(y_test, y_pred_decision_tree)\n",
    "confusion_matrix_decision_tree = confusion_matrix(y_test, y_pred_decision_tree)\n",
    "\n",
    "fpr_decision_tree, tpr_decision_tree, thresholds_decision_tree = roc_curve(y_test, y_pred_decision_tree)\n",
    "roc_auc_decision_tree = auc(fpr_decision_tree, tpr_decision_tree)\n",
    "\n",
    "print(\"Accuracy of Decision Tree model on Test Data:\", accuracy_decision_tree)\n",
    "print(\"Precision of Decision Tree model on Test Data:\", precision_decision_tree)\n",
    "print(\"Recall of Decision Tree model on Test Data:\", recall_decision_tree)\n",
    "print(\"F1 Score of Decision Tree model on Test Data:\", f1score_decision_tree)\n",
    "print(\"AUC of Decision Tree model on Test Data:\", AUC_decision_tree)\n",
    "print(\"Confusion Matrix of Decision Tree model on Test Data:\")\n",
    "print( confusion_matrix_decision_tree)\n",
    "\n",
    "\n",
    "# Train and test a Logistic Regression model using the best features\n",
    "clf_logistic_regression = LogisticRegression(random_state=42)\n",
    "clf_logistic_regression.fit(X_train_resampled.iloc[:, best_features], y_train_resampled)\n",
    "y_pred_logistic_regression = clf_logistic_regression.predict(X_test.iloc[:, best_features])\n",
    "\n",
    "# Evaluate Logistic Regression model\n",
    "accuracy_logistic_regression = accuracy_score(y_test, y_pred_logistic_regression)\n",
    "precision_logistic_regression = precision_score(y_test, y_pred_logistic_regression)\n",
    "recall_logistic_regression = recall_score(y_test, y_pred_logistic_regression)\n",
    "f1_logistic_regression = f1_score(y_test, y_pred_logistic_regression)\n",
    "auc_logistic_regression = roc_auc_score(y_test, y_pred_logistic_regression)\n",
    "confusion_logistic_regression = confusion_matrix(y_test, y_pred_logistic_regression)\n",
    "\n",
    "fpr_logistic_regression, tpr_logistic_regression, thresholds_logistic_regression = roc_curve(y_test, y_pred_logistic_regression)\n",
    "roc_auc_logistic_regression = auc(fpr_logistic_regression, tpr_logistic_regression)\n",
    "\n",
    "\n",
    "print(\"Accuracy of Decision Tree model on Test Data:\", accuracy_logistic_regression)\n",
    "print(\"Precision of Decision Tree model on Test Data:\", precision_logistic_regression)\n",
    "print(\"Recall of Decision Tree model on Test Data:\", recall_logistic_regression)\n",
    "print(\"F1 Score of Decision Tree model on Test Data:\", f1_logistic_regression)\n",
    "print(\"AUC of Decision Tree model on Test Data:\", auc_logistic_regression)\n",
    "print(\"Confusion Matrix of Decision Tree model on Test Data:\")\n",
    "print(confusion_logistic_regression)\n",
    "\n",
    "\n",
    "# Train and test a Naive Bayes model using the best features\n",
    "clf_naive_bayes = GaussianNB()\n",
    "clf_naive_bayes.fit(X_train_resampled.iloc[:, best_features], y_train_resampled)\n",
    "y_pred_naive_bayes = clf_naive_bayes.predict(X_test.iloc[:, best_features])\n",
    "\n",
    "# Evaluate Naive Bayes model\n",
    "accuracy_naive_bayes = accuracy_score(y_test, y_pred_naive_bayes)\n",
    "precision_naive_bayes = precision_score(y_test, y_pred_naive_bayes)\n",
    "recall_naive_bayes = recall_score(y_test, y_pred_naive_bayes)\n",
    "f1_naive_bayes = f1_score(y_test, y_pred_naive_bayes)\n",
    "auc_naive_bayes = roc_auc_score(y_test, y_pred_naive_bayes)\n",
    "confusion_naive_bayes = confusion_matrix(y_test, y_pred_naive_bayes)\n",
    "\n",
    "fpr_naive_bayes, tpr_naive_bayes, thresholds_naive_bayes = roc_curve(y_test, y_pred_naive_bayes)\n",
    "roc_auc_naive_bayes = roc_auc_score(y_test, y_pred_naive_bayes)\n",
    "\n",
    "print(\"Metrics for Naive Bayes Model:\")\n",
    "print(\"Accuracy:\", accuracy_naive_bayes)\n",
    "print(\"Precision:\", precision_naive_bayes)\n",
    "print(\"Recall:\", recall_naive_bayes)\n",
    "print(\"F1 Score:\", f1_naive_bayes)\n",
    "print(\"AUC:\", auc_naive_bayes)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_naive_bayes)\n",
    "\n",
    "\n",
    "# Train and test a Random Forest model using the best features\n",
    "clf_random_forest = RandomForestClassifier(random_state=42)\n",
    "clf_random_forest.fit(X_train_resampled.iloc[:, best_features], y_train_resampled)\n",
    "y_pred_random_forest = clf_random_forest.predict(X_test.iloc[:, best_features])\n",
    "\n",
    "# Evaluate Random Forest model\n",
    "accuracy_random_forest = accuracy_score(y_test, y_pred_random_forest)\n",
    "precision_random_forest = precision_score(y_test, y_pred_random_forest)\n",
    "recall_random_forest = recall_score(y_test, y_pred_random_forest)\n",
    "f1_random_forest = f1_score(y_test, y_pred_random_forest)\n",
    "auc_random_forest = roc_auc_score(y_test, y_pred_random_forest)\n",
    "confusion_random_forest = confusion_matrix(y_test, y_pred_random_forest)\n",
    "\n",
    "fpr_random_forest, tpr_random_forest, thresholds_random_forest = roc_curve(y_test, y_pred_random_forest)\n",
    "roc_auc_random_forest = roc_auc_score(y_test, y_pred_random_forest)\n",
    "\n",
    "\n",
    "print(\"Metrics for Random Forest Model:\")\n",
    "print(\"Accuracy:\", accuracy_random_forest)\n",
    "print(\"Precision:\", precision_random_forest)\n",
    "print(\"Recall:\", recall_random_forest)\n",
    "print(\"F1 Score:\", f1_random_forest)\n",
    "print(\"AUC:\", auc_random_forest)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_random_forest)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(fpr_decision_tree, tpr_decision_tree, color='darkorange', lw=2, label='Decision Tree (AUC = {:.2f})'.format(roc_auc_decision_tree))\n",
    "plt.plot(fpr_logistic_regression, tpr_logistic_regression, color='darkgreen', lw=2, label='Logistic Regression (AUC = {:.2f})'.format(roc_auc_logistic_regression))\n",
    "plt.plot(fpr_naive_bayes, tpr_naive_bayes, color='purple', lw=2, label='Naive Bayes (AUC = {:.2f})'.format(roc_auc_naive_bayes))\n",
    "plt.plot(fpr_random_forest, tpr_random_forest, color='blue', lw=2, label='Random Forest (AUC = {:.2f})'.format(roc_auc_random_forest))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Random classifier\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming 'Time' is the feature representing seconds elapsed since the first transaction\n",
    "data['Time'] = pd.to_datetime(data['Time'], unit='s')\n",
    "\n",
    "# Extract 'Hour' feature\n",
    "data['Hour'] = data['Time'].dt.hour\n",
    "\n",
    "# KDE Plot for Hour\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.kdeplot(data.loc[data['Class'] == 0, 'Hour'], label='Valid', shade=True, color='green')\n",
    "sns.kdeplot(data.loc[data['Class'] == 1, 'Hour'], label='Fraud', shade=True, color='red')\n",
    "plt.title('KDE Plot for Hour of the Day')\n",
    "plt.xlabel('Hour of the Day')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
